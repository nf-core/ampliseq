{
    "$schema": "http://json-schema.org/draft-07/schema",
    "$id": "https://raw.githubusercontent.com/nf-core/ampliseq/master/nextflow_schema.json",
    "title": "nf-core/ampliseq pipeline parameters",
    "description": "Amplicon sequencing analysis workflow using DADA2 and QIIME2",
    "type": "object",
    "definitions": {
        "main_arguments": {
            "title": "Main arguments",
            "type": "object",
            "description": "",
            "default": "",
            "properties": {
                "input": {
                    "type": "string",
                    "fa_icon": "fas fa-dna",
                    "description": "Either a tab-seperated sample sheet, a fasta file, or a folder containing zipped FastQ files",
                    "help_text": "Points to the main pipeline input, one of the following:\n- folder containing compressed fastq files\n- sample sheet ending with `.tsv` that points towards compressed fastq files\n- fasta file ending with `.fasta`, `.fna` or `.fa` that will be taxonomically classified\n\nRelated parameters are:\n- `--pacbio` and `--iontorrent` if the sequencing data is PacBio data or IonTorrent data (default expected: paired-end Illumina data)\n- `--single_end` if the sequencing data is single-ended Illumina data (default expected: paired-end Illumina data)\n- `--multiple_sequencing_runs` (folder input only) if the sequencing data originates from multiple sequencing runs\n- `--extension` (folder input only) if the sequencing file names do not follow the default (`\"/*_R{1,2}_001.fastq.gz\"`)\n- `--dada_ref_taxonomy` and `--qiime_ref_taxonomy` to choose an appropriate reference taxonomy for the type of amplicon (16S/18S/ITS) (default: 16S rRNA sequence database)\n\n##### Folder containing zipped FastQ files\n\nFor example:\n\n```bash\n--input 'path/to/data'\n```\n\nExample for input data organization from one sequencing run with two samples, paired-end data:\n\n```bash\ndata\n  ├─sample1_1_L001_R1_001.fastq.gz\n  ├─sample1_1_L001_R2_001.fastq.gz\n  ├─sample2_1_L001_R1_001.fastq.gz\n  └─sample2_1_L001_R2_001.fastq.gz\n```\n\nPlease note the following requirements:\n\n1. The path must be enclosed in quotes\n2. The folder must contain gzip compressed demultiplexed fastq files. If the file names do not follow the default (`\"/*_R{1,2}_001.fastq.gz\"`), please check `--extension`.\n3. Sample identifiers are extracted from file names, i.e. the string before the first underscore `_`, these must be unique\n4. If your data is scattered, produce a sample sheet\n5. All sequencing data should originate from one sequencing run, because processing relies on run-specific error models that are unreliable when data from several sequencing runs are mixed. Sequencing data originating from multiple sequencing runs requires additionally the parameter `--multiple_sequencing_runs` and a specific folder structure.\n\n##### Sample sheet\n\nThe sample sheet file is an alternative way to provide input reads, it must be a tab-separated file ending with `.tsv` that must have two to four columns with the following headers: \n- `sampleID` (required): Unique sample identifiers, any unique string (may not contain dots `.`, must not start with a number when using metadata)\n- `forwardReads` (required): Paths to (forward) reads zipped FastQ files\n- `reverseReads` (optional): Paths to reverse reads zipped FastQ files, required if the data is paired-end\n- `run` (optional): If the data was produced by multiple sequencing runs, any string\n\nFor example:\n\n```bash\n--input 'path/to/samplesheet.tsv'\n```\n\n##### Fasta file\n\nWhen pointing at a file ending with `.fasta`, `.fna` or `.fa`, the containing sequences will be taxonomically classified. All other pipeline steps will be skipped.\n\nThis can be used to taxonomically classify previously produced ASV/OTU sequences.\n\nFor example:\n\n```bash\n--input 'path/to/amplicon_sequences.fasta'\n```"
                },
                "FW_primer": {
                    "type": "string",
                    "description": "Forward primer sequence",
                    "help_text": "In amplicon sequencing methods, PCR with specific primers produces the amplicon of interest. These primer sequences need to be trimmed from the reads before further processing and are also required for producing an appropriate classifier. Do not use here any technical sequence such as adapter sequences but only the primer sequence that matches the biological amplicon.\n\nFor example:\n\n```bash\n--FW_primer \"GTGYCAGCMGCCGCGGTAA\" --RV_primer \"GGACTACNVGGGTWTCTAAT\"\n```",
                    "fa_icon": "fas fa-arrow-circle-right"
                },
                "RV_primer": {
                    "type": "string",
                    "description": "Reverse primer sequence",
                    "help_text": "In amplicon sequencing methods, PCR with specific primers produces the amplicon of interest. These primer sequences need to be trimmed from the reads before further processing and are also required for producing an appropriate classifier. Do not use here any technical sequence such as adapter sequences but only the primer sequence that matches the biological amplicon.\n\nFor example:\n\n```bash\n--FW_primer GTGYCAGCMGCCGCGGTAA --RV_primer GGACTACNVGGGTWTCTAAT\n```",
                    "fa_icon": "fas fa-arrow-alt-circle-left"
                },
                "metadata": {
                    "type": "string",
                    "description": "Path to metadata sheet, when missing most downstream analysis are skipped (barplots, PCoA plots, ...).",
                    "help_text": "This is optional, but for performing downstream analysis such as barplots, diversity indices or differential abundance testing, a metadata file is essential.\n\nRelated parameter:\n- `--metadata_category` (optional) to choose columns that are used for testing significance\n\nFor example:\n\n```bash\n--metadata \"path/to/metadata.tsv\"\n```\n\nPlease note the following requirements:\n\n1. The path must be enclosed in quotes\n2. The metadata file has to follow the QIIME2 specifications (https://docs.qiime2.org/2021.2/tutorials/metadata/)\n\nThe first column in the tab-separated metadata file is the sample identifier column (required header: `ID`) and defines the sample or feature IDs associated with your study. Metadata files are not required to have additional metadata columns, so a file containing only an ID column is a valid QIIME 2 metadata file. Additional columns defining metadata associated with each sample or feature ID are optional.\n**NB**: without additional columns there might be no groupings for the downstream analyses.\n\nSample identifiers should be 36 characters long or less, and also contain only ASCII alphanumeric characters (i.e. in the range of [a-z], [A-Z], or [0-9]), or the dash (-) character. For downstream analysis, by default all numeric columns, blanks or NA are removed, and only columns with multiple different values but not all unique are selected.\n\nThe columns which are to be assessed can be specified by `--metadata_category`. If `--metadata_category` isn't specified than all columns that fit the specification are automatically chosen.",
                    "fa_icon": "fas fa-file-csv"
                }
            },
            "required": [
                "input",
                "FW_primer",
                "RV_primer"
            ],
            "fa_icon": "fas fa-terminal"
        },
        "other_input_output_options": {
            "title": "Other input/output options",
            "type": "object",
            "fa_icon": "fas fa-terminal",
            "description": "Define where the pipeline should find input data and save output data.",
            "properties": {
                "pacbio": {
                    "type": "boolean",
                    "description": "If data is single-ended PacBio reads instead of Illumina"
                },
                "iontorrent": {
                    "type": "boolean",
                    "description": "If data is single-ended IonTorrent reads instead of Illumina"
                },
                "single_end": {
                    "type": "boolean",
                    "description": "If data is single-ended Illumina reads instead of paired-end"
                },
                "cut_its": {
                    "type": "boolean",
                    "description": "If data is long read ITS sequences, that need to be cut to ITS region only for taxonomy assignment"
                },
                "multiple_sequencing_runs": {
                    "type": "boolean",
                    "description": "If samples were sequenced in multiple sequencing runs",
                    "help_text": "Expects one sub-folder per sequencing run in the folder specified by `--input` containing sequencing data of the specific run.\nSample identifiers are taken from sequencing files, specifically the string before the first underscore will be the sample ID. Sample IDs across all sequencing runs (all sequencing files) have to be unique. If this is not the case, please use a sample sheet as input instead.\n\nExample for input data organization:\n\n```bash\ndata\n  |-run1\n  |  |-sample1_1_L001_R1_001.fastq.gz\n  |  |-sample1_1_L001_R2_001.fastq.gz\n  |  |-sample2_1_L001_R1_001.fastq.gz\n  |  |-sample2_1_L001_R2_001.fastq.gz\n  |\n  |-run2\n     |-sample3_1_L001_R1_001.fastq.gz\n     |-sample3_1_L001_R2_001.fastq.gz\n     |-sample4_1_L001_R1_001.fastq.gz\n     |-sample4_1_L001_R2_001.fastq.gz\n```\n\nExample command to analyze this data in one pipeline run:\n\n```bash\nnextflow run nf-core/ampliseq \\\n    -profile singularity \\\n    --input \"data\" \\\n    --FW_primer \"GTGYCAGCMGCCGCGGTAA\" \\\n    --RV_primer \"GGACTACNVGGGTWTCTAAT\" \\\n    --metadata \"data/Metadata.tsv\" \\\n    --multiple_sequencing_runs\n```",
                    "fa_icon": "fas fa-running"
                },
                "illumina_pe_its": {
                    "type": "boolean",
                    "description": "If analysing ITS amplicons or any other region with large length variability with Illumina paired end reads",
                    "help_text": "This will cause the pipeline to\n- not truncate input reads if not `--trunclenf` and `--trunclenr` are overwriting defaults\n- remove reverse complement primers from the end of reads in case the read length exceeds the amplicon length"
                },
                "concatenate_reads": {
                    "type": "boolean",
                    "description": "Not recommended: When paired end reads are not sufficiently overlapping for merging.",
                    "help_text": "This parameters specifies that paired-end reads are not merged after denoising but concatenated (separated by 10 N's). This is of advantage when an amplicon was sequenced that is too long for merging (i.e. bad experimental design). This is an alternative to only analyzing the forward or reverse read in case of non-overlapping paired-end sequencing data.\n\n**This parameter is not recommended! Only if all other options fail.**"
                },
                "sample_inference": {
                    "type": "string",
                    "default": "independent",
                    "help_text": "If samples are treated independent (lowest sensitivity and lowest resources), pooled (highest sensitivity and resources) or pseudo-pooled (balance between required resources and sensitivity).",
                    "description": "Mode of sample inference: \"independent\", \"pooled\" or \"pseudo\"",
                    "enum": [
                        "independent",
                        "pooled",
                        "pseudo"
                    ]
                },
                "metadata_category": {
                    "type": "string",
                    "description": "Comma separated list of metadata column headers for statistics.",
                    "help_text": "Here columns in the metadata sheet can be chosen with groupings that are used for diversity indices and differential abundance analysis. By default, all suitable columns in the metadata sheet will be used if this option is not specified. Suitable are columns which are categorical (not numerical) and have multiple different values which are not all unique. For example:\n\n```bash\n--metadata_category \"treatment1,treatment2\"\n```\n\nPlease note the following requirements:\n\n1. Comma separated list enclosed in quotes\n2. May not contain whitespace characters\n3. Each comma separated term has to match exactly one column name in the metadata sheet"
                },
                "qiime_adonis_formula": {
                    "type": "string",
                    "description": "Formula for QIIME2 ADONIS metadata feature importance test for beta diversity distances",
                    "help_text": "Model formula containing only independent terms contained in the sample metadata. These can be continuous variables or factors, and they can have interactions as in a typical R formula. Essentially, columns in the metadata sheet can be chosen that have no empty values, not only unique values, or not only identical values.\nFor example, \"treatment1+treatment2\" tests whether the data partitions based on \"treatment1\" and \"treatment2\" sample metadata. \"treatment1*treatment2\" test both of those effects as well as their interaction.\nMore examples can be found in the R documentation, https://cran.r-project.org/doc/manuals/r-release/R-intro.html#Formulae-for-statistical-models"
                },
                "extension": {
                    "type": "string",
                    "default": "/*_R{1,2}_001.fastq.gz",
                    "description": "Naming of sequencing files",
                    "help_text": "Indicates the naming of sequencing files (default: `\"/*_R{1,2}_001.fastq.gz\"`).\n\nPlease note:\n\n1. The prepended slash (`/`) is required\n2. The star (`*`) is the required wildcard for sample names\n3. The curly brackets (`{}`) enclose the orientation for paired end reads, seperated by a comma (`,`).\n4. The pattern must be enclosed in quotes\n\nFor example for one sample (name: `1`) with forward (file: `1_a.fastq.gz`) and reverse (file: `1_b.fastq.gz`) reads in folder `data`:\n\n```bash\n--input \"data\" --extension \"/*_{a,b}.fastq.gz\"\n```"
                },
                "picrust": {
                    "type": "boolean",
                    "description": "If the functional potential of the bacterial community is predicted."
                },
                "sbdiexport": {
                    "type": "boolean",
                    "description": "If data should be exported in SBDI (Swedish biodiversity infrastructure) Excel format."
                },
                "outdir": {
                    "type": "string",
                    "description": "Path to the output directory where the results will be saved.",
                    "default": "./results",
                    "fa_icon": "fas fa-folder-open"
                },
                "email": {
                    "type": "string",
                    "description": "Email address for completion summary.",
                    "fa_icon": "fas fa-envelope",
                    "help_text": "Set this parameter to your e-mail address to get a summary e-mail with details of the run sent to you when the workflow exits. If set in your user config file (`~/.nextflow/config`) then you don't need to specify this on the command line for every run.",
                    "pattern": "^([a-zA-Z0-9_\\-\\.]+)@([a-zA-Z0-9_\\-\\.]+)\\.([a-zA-Z]{2,5})$"
                }
            }
        },
        "cutoffs": {
            "title": "Cutoffs",
            "type": "object",
            "description": "",
            "default": "",
            "properties": {
                "retain_untrimmed": {
                    "type": "boolean",
                    "description": "Cutadapt will retain untrimmed reads, choose only if input reads are not expected to contain primer sequences.",
                    "help_text": "When read sequences are trimmed, untrimmed read pairs are discarded routinely. Use this option to retain untrimmed read pairs. This is usually not recommended and is only of advantage for specific protocols that prevent sequencing PCR primers. ",
                    "fa_icon": "far fa-plus-square"
                },
                "double_primer": {
                    "type": "boolean",
                    "description": "Cutadapt will be run twice to ensure removal of potential double primers",
                    "help_text": "Cutdapt will be run twice, first to remove reads without primers (default), then a second time to remove reads that erroneously contain a second set of primers, not to be used with `--retain_untrimmed`.",
                    "fa_icon": "fas fa-project-diagram"
                },
                "trunclenf": {
                    "type": "integer",
                    "description": "DADA2 read truncation value for forward strand, set this to 0 for no truncation",
                    "help_text": "Read denoising by DADA2 creates an error profile specific to a sequencing run and uses this to correct sequencing errors. This method prefers when all reads to have the same length and as high quality as possible while maintaining at least 20 bp overlap for merging. One cutoff for the forward read `--trunclenf` and one for the reverse read `--trunclenr` truncate all longer reads at that position and drop all shorter reads.\nIf not set, these cutoffs will be determined automatically for the position before the mean quality score drops below `--trunc_qmin`.\n\nFor example:\n\n```bash\n--trunclenf 180 --trunclenr 120\n```\n\nPlease note:\n\n1. Overly aggressive truncation might lead to insufficient overlap for read merging\n2. Too little truncation might reduce denoised reads\n3. The code choosing these values automatically cannot take the points above into account, therefore checking read numbers is essential",
                    "fa_icon": "fas fa-ban"
                },
                "trunclenr": {
                    "type": "integer",
                    "description": "DADA2 read truncation value for reverse strand, set this to 0 for no truncation",
                    "help_text": "Read denoising by DADA2 creates an error profile specific to a sequencing run and uses this to correct sequencing errors. This method prefers when all reads to have the same length and as high quality as possible while maintaining at least 20 bp overlap for merging. One cutoff for the forward read `--trunclenf` and one for the reverse read `--trunclenr` truncate all longer reads at that position and drop all shorter reads.\nIf not set, these cutoffs will be determined automatically for the position before the mean quality score drops below `--trunc_qmin`.\n\nFor example:\n\n```bash\n--trunclenf 180 --trunclenr 120\n```\n\nPlease note:\n\n1. Overly aggressive truncation might lead to insufficient overlap for read merging\n2. Too little truncation might reduce denoised reads\n3. The code choosing these values automatically cannot take the points above into account, therefore checking read numbers is essential",
                    "fa_icon": "fas fa-ban"
                },
                "trunc_qmin": {
                    "type": "integer",
                    "default": 25,
                    "description": "If --trunclenf and --trunclenr are not set, these values will be automatically determined using this median quality score",
                    "help_text": "Automatically determine `--trunclenf` and `--trunclenr` before the median quality score drops below `--trunc_qmin`. The fraction of reads retained is defined by `--trunc_rmin`, which might override the quality cutoff.\n\nFor example:\n\n```bash\n--trunc_qmin 35\n```\n\nPlease note:\n\n1. The code choosing `--trunclenf` and `--trunclenr` using `--trunc_qmin` automatically cannot take amplicon length or overlap requirements for merging into account, therefore use with caution.\n2. A minimum value of 25 is recommended. However, high quality data with a large paired sequence overlap might justify a higher value (e.g. 35). Also, very low quality data might require a lower value.\n3. If the quality cutoff is too low to include a certain fraction of reads that is specified by `--trunc_rmin` (e.g. 0.75 means at least 75% percent of reads are retained), a lower cutoff according to `--trunc_rmin` superseeds the quality cutoff.",
                    "fa_icon": "fas fa-greater-than-equal"
                },
                "trunc_rmin": {
                    "type": "number",
                    "default": 0.75,
                    "description": "Assures that values chosen with --trunc_qmin will retain a fraction of reads.",
                    "help_text": "Value can range from 0 to 1. 0 means no reads need to be retained and 1 means all reads need to be retained. The minimum lengths of --trunc_qmin and --trunc_rmin are chosen as DADA2 cutoffs.",
                    "minimum": 0,
                    "maximum": 1
                },
                "max_ee": {
                    "type": "integer",
                    "default": 2,
                    "description": "DADA2 read filtering option",
                    "help_text": "After truncation, reads with higher than `max_ee` \"expected errors\" will be discarded. In case of very long reads, you might want to increase this value.  We recommend (to start with) a value corresponding to approximately 1 expected error per 100-200 bp (default: 2)",
                    "fa_icon": "fas fa-equals"
                },
                "max_len": {
                    "type": "integer",
                    "description": "DADA2 read filtering option",
                    "fa_icon": "fas fa-less-than-equal",
                    "help_text": "Remove reads with length greater than `max_len` after trimming and truncation. Must be a positive integer."
                },
                "min_len": {
                    "type": "integer",
                    "default": 50,
                    "description": "DADA2 read filtering option",
                    "fa_icon": "fas fa-greater-than-equal",
                    "help_text": "Remove reads with length less than `min_len` after trimming and truncation."
                },
                "dada_tax_agglom_min": {
                    "type": "integer",
                    "default": 2,
                    "description": "Minimum taxonomy agglomeration level for DADA2 classification",
                    "fa_icon": "fas fa-greater-than-equal",
                    "help_text": "Depends on the reference taxonomy database used."
                },
                "dada_tax_agglom_max": {
                    "type": "integer",
                    "default": 7,
                    "description": "Maximum taxonomy agglomeration level for DADA2 classification",
                    "fa_icon": "fas fa-greater-than-equal",
                    "help_text": "Depends on the reference taxonomy database used. Default databases should have genus level at 7."
                },
                "qiime_tax_agglom_min": {
                    "type": "integer",
                    "default": 2,
                    "description": "Minimum taxonomy agglomeration level for QIIME2 classification",
                    "fa_icon": "fas fa-greater-than-equal",
                    "help_text": "Depends on the reference taxonomy database used."
                },
                "qiime_tax_agglom_max": {
                    "type": "integer",
                    "default": 6,
                    "description": "Maximum taxonomy agglomeration level for QIIME2 classification",
                    "fa_icon": "fas fa-greater-than-equal",
                    "help_text": "Depends on the reference taxonomy database used. Default databases should have genus level at 6."
                }
            },
            "fa_icon": "fas fa-filter"
        },
        "taxonomic_database": {
            "title": "Taxonomic database",
            "type": "object",
            "description": "",
            "default": "",
            "properties": {
                "dada_ref_taxonomy": {
                    "type": "string",
                    "help_text": "Choose any of the supported databases, and optionally also specify the version. Database and version are separated by an equal sign (`=`, e.g. `silva=138`) . This will download the desired database, format it to produce a file that is compatible with DADA2's assignTaxonomy and another file that is compatible with DADA2's addSpecies.\n\nThe following databases are supported:\n- GTDB - Genome Taxonomy Database - 16S rRNA\n- PR2 - Protist Reference Ribosomal Database - 18S rRNA\n- RDP - Ribosomal Database Project  - 16S rRNA\n- SILVA ribosomal RNA gene database project  - 16S rRNA\n- UNITE - eukaryotic nuclear ribosomal ITS region  - ITS\n\nGenerally, using `gtdb`, `pr2`, `rdp`, `sbdi-gtdb`, `silva`, `unite-fungi`, or `unite-alleuk` will select the most recent supported version. For details on what values are valid, please either use an invalid value such as `x` (causing the pipeline to send an error message with a list of all valid values) or see `conf/ref_databases.config`.\n\nPlease note that commercial/non-academic entities [require licensing](https://www.arb-silva.de/silva-license-information) for SILVA v132 database (non-default) but not from v138 on (default).",
                    "description": "Name of supported database, and optionally also version number",
                    "default": "silva=138",
                    "enum": [
                        "gtdb=05-RS95",
                        "gtdb",
                        "pr2=4.14.0",
                        "pr2=4.13.0",
                        "pr2",
                        "rdp=18",
                        "rdp",
                        "sbdi-gtdb",
                        "sbdi-gtdb=R06-RS202-3",
                        "sbdi-gtdb=R06-RS202-1",
                        "silva=132",
                        "silva=138",
                        "silva",
                        "unite-fungi=8.3",
                        "unite-fungi=8.2",
                        "unite-fungi",
                        "unite-alleuk=8.3",
                        "unite-alleuk=8.2",
                        "unite-alleuk"
                    ]
                },
                "cut_dada_ref_taxonomy": {
                    "type": "boolean",
                    "help_text": "Expected amplified sequences are extracted from the DADA2 reference taxonomy using the primer sequences, that might improve classification. This is not applied to species classification (assignSpecies) but only for lower taxonomic levels (assignTaxonomy).",
                    "description": "If the expected amplified sequences are extracted from the DADA2 reference taxonomy database"
                },
                "qiime_ref_taxonomy": {
                    "type": "string",
                    "help_text": "Choose any of the supported databases, and optionally also specify the version. Database and version are separated by an equal sign (`=`, e.g. `silva=138`) . This will download the desired database and initiate taxonomic classification with QIIME2 and the chosen database.\n\nIf both, `--dada_ref_taxonomy` and `--qiime_ref_taxonomy` are used, DADA2 classification will be used for downstream analysis.\n\nThe following databases are supported:\n- SILVA ribosomal RNA gene database project - 16S rRNA\n- UNITE - eukaryotic nuclear ribosomal ITS region - ITS\n- Greengenes (only testing!)\n\nGenerally, using `silva`, `unite-fungi`, or `unite-alleuk` will select the most recent supported version. For testing purposes, the tiny database `greengenes85` (dereplicated at 85% sequence similarity) is available. For details on what values are valid, please either use an invalid value such as `x` (causing the pipeline to send an error message with all valid values) or see `conf/ref_databases.config`.",
                    "description": "Name of supported database, and optionally also version number",
                    "enum": [
                        "silva=138",
                        "silva",
                        "unite-fungi=8.2",
                        "unite-fungi",
                        "unite-alleuk=8.2",
                        "unite-alleuk",
                        "greengenes85"
                    ]
                },
                "classifier": {
                    "type": "string",
                    "description": "Path to QIIME2 trained classifier file (typically *-classifier.qza)",
                    "help_text": "If you have trained a compatible classifier before, from sources such as SILVA (https://www.arb-silva.de/), Greengenes (http://greengenes.secondgenome.com/downloads) or RDP (https://rdp.cme.msu.edu/). \n\nFor example:\n\n```bash\n--classifier \"FW_primer-RV_primer-classifier.qza\"\n```\n\nPlease note the following requirements:\n\n1. The path must be enclosed in quotes\n2. The classifier is a Naive Bayes classifier produced by `qiime feature-classifier fit-classifier-naive-bayes` (e.g. by this pipeline)\n3. The primer pair for the amplicon PCR and the computing of the classifier are exactly the same (or full-length, potentially lower performance)\n4. The classifier has to be trained by the same version of scikit-learn as this version of the pipeline uses"
                }
            },
            "fa_icon": "fas fa-database"
        },
        "filtering": {
            "title": "Filtering",
            "type": "object",
            "description": "",
            "default": "",
            "properties": {
                "ignore_empty_input_files": {
                    "type": "boolean",
                    "default": false,
                    "description": "Ignore input files considered too small (<1KB) for individual samples and continue the pipeline without those samples."
                },
                "ignore_failed_trimming": {
                    "type": "boolean",
                    "default": false,
                    "description": "Ignore files considered too small (<1KB) after trimming and continue the pipeline without those samples."
                },
                "exclude_taxa": {
                    "type": "string",
                    "default": "mitochondria,chloroplast",
                    "description": "Comma separated list of unwanted taxa, to skip taxa filtering use \"none\"",
                    "help_text": "Depending on the primers used, PCR might amplify unwanted or off-target DNA. By default sequences originating from mitochondria or chloroplasts are removed. The taxa specified are excluded from further analysis.\nFor example to exclude any taxa that contain mitochondria, chloroplast, or archaea:\n\n```bash\n--exclude_taxa \"mitochondria,chloroplast,archaea\"\n```\n\nIf you prefer not filtering the data, specify:\n\n```bash\n--exclude_taxa \"none\"\n```\n\nPlease note the following requirements:\n\n1. Comma separated list enclosed in quotes\n2. May not contain whitespace characters\n3. Features that contain one or several of these terms in their taxonomical classification are excluded from further analysis\n4. The taxonomy level is not taken into consideration"
                },
                "min_frequency": {
                    "type": "integer",
                    "default": 1,
                    "description": "Abundance filtering",
                    "help_text": "Remove entries from the feature table below an absolute abundance threshold (default: 1, meaning filter is disabled). Singletons are often regarded as artifacts, choosing a value of 2 removes sequences with less than 2 total counts from the feature table.\n\nFor example to remove singletons choose:\n\n```bash\n--min_frequency 2\n```"
                },
                "min_samples": {
                    "type": "integer",
                    "default": 1,
                    "description": "Prevalence filtering",
                    "help_text": "Filtering low prevalent features from the feature table, e.g. keeping only features that are present in at least two samples can be achived by choosing a value of 2 (default: 1, meaning filter is disabled). Typically only used when having replicates for all samples.\n\nFor example to retain features that are present in at least two sample:\n\n```bash\n--min_samples 2\n```\n\nPlease note this is independent of abundance."
                }
            },
            "fa_icon": "fas fa-filter"
        },
        "skipping_specific_steps": {
            "title": "Skipping specific steps",
            "type": "object",
            "description": "",
            "default": "",
            "properties": {
                "skip_fastqc": {
                    "type": "boolean",
                    "default": false,
                    "description": "Skip FastQC"
                },
                "skip_qiime": {
                    "type": "boolean",
                    "default": false,
                    "description": "Skip all steps that are executed by QIIME2, including QIIME2 software download, taxonomy assignment by QIIME2, barplots, relative abundance tables, diversity analysis, differential abundance testing."
                },
                "skip_taxonomy": {
                    "type": "boolean",
                    "default": false,
                    "description": "Skip taxonomic classification. Incompatible with `--sbdiexport`"
                },
                "skip_dada_addspecies": {
                    "type": "boolean",
                    "default": false,
                    "description": "Skip species level when using DADA2 for taxonomic classification. This reduces the required memory dramatically under certain conditions. Incompatible with `--sbdiexport`"
                },
                "skip_barplot": {
                    "type": "boolean",
                    "default": false,
                    "description": "Skip producing barplot"
                },
                "skip_abundance_tables": {
                    "type": "boolean",
                    "default": false,
                    "description": "Skip producing any relative abundance tables"
                },
                "skip_alpha_rarefaction": {
                    "type": "boolean",
                    "default": false,
                    "description": "Skip alpha rarefaction"
                },
                "skip_diversity_indices": {
                    "type": "boolean",
                    "default": false,
                    "description": "Skip alpha and beta diversity analysis"
                },
                "skip_ancom": {
                    "type": "boolean",
                    "default": false,
                    "description": "Skip differential abundance testing"
                },
                "skip_multiqc": {
                    "type": "boolean",
                    "default": false,
                    "description": "Skip MultiQC reporting"
                }
            }
        },
        "institutional_config_options": {
            "title": "Institutional config options",
            "type": "object",
            "fa_icon": "fas fa-university",
            "description": "Parameters used to describe centralised config profiles. These should not be edited.",
            "help_text": "The centralised nf-core configuration profiles use a handful of pipeline parameters to describe themselves. This information is then printed to the Nextflow log when you run a pipeline. You should not need to change these values when you run a pipeline.",
            "properties": {
                "custom_config_version": {
                    "type": "string",
                    "description": "Git commit id for Institutional configs.",
                    "default": "master",
                    "hidden": true,
                    "fa_icon": "fas fa-users-cog"
                },
                "custom_config_base": {
                    "type": "string",
                    "description": "Base directory for Institutional configs.",
                    "default": "https://raw.githubusercontent.com/nf-core/configs/master",
                    "hidden": true,
                    "help_text": "If you're running offline, Nextflow will not be able to fetch the institutional config files from the internet. If you don't need them, then this is not a problem. If you do need them, you should download the files from the repo and tell Nextflow where to find them with this parameter.",
                    "fa_icon": "fas fa-users-cog"
                },
                "config_profile_name": {
                    "type": "string",
                    "description": "Institutional config name.",
                    "hidden": true,
                    "fa_icon": "fas fa-users-cog"
                },
                "config_profile_description": {
                    "type": "string",
                    "description": "Institutional config description.",
                    "hidden": true,
                    "fa_icon": "fas fa-users-cog"
                },
                "config_profile_contact": {
                    "type": "string",
                    "description": "Institutional config contact information.",
                    "hidden": true,
                    "fa_icon": "fas fa-users-cog"
                },
                "config_profile_url": {
                    "type": "string",
                    "description": "Institutional config URL link.",
                    "hidden": true,
                    "help_text": "By default, parameters set as _hidden_ in the schema are not shown on the command line when a user runs with `--help`. Specifying this option will tell the pipeline to show all parameters."
                },
                "multiqc_title": {
                    "type": "string",
                    "description": "MultiQC report title. Printed as page header, used for filename if not otherwise specified.",
                    "hidden": true,
                    "fa_icon": "fas fa-file-signature"
                }
            }
        },
        "generic_options": {
            "title": "Generic options",
            "type": "object",
            "fa_icon": "fas fa-file-import",
            "description": "Less common options for the pipeline, typically set in a config file.",
            "help_text": "These options are common to all nf-core pipelines and allow you to customise some of the core preferences for how the pipeline runs.\n\nTypically these options would be set in a Nextflow config file loaded for all pipeline runs, such as `~/.nextflow/config`.",
            "properties": {
                "help": {
                    "type": "boolean",
                    "description": "Display help text.",
                    "fa_icon": "fas fa-question-circle",
                    "hidden": true
                },
                "email_on_fail": {
                    "type": "string",
                    "description": "Email address for completion summary, only when pipeline fails.",
                    "fa_icon": "fas fa-exclamation-triangle",
                    "pattern": "^([a-zA-Z0-9_\\-\\.]+)@([a-zA-Z0-9_\\-\\.]+)\\.([a-zA-Z]{2,5})$",
                    "help_text": "An email address to send a summary email to when the pipeline is completed - ONLY sent if the pipeline does not exit successfully.",
                    "hidden": true
                },
                "plaintext_email": {
                    "type": "boolean",
                    "description": "Send plain-text email instead of HTML.",
                    "fa_icon": "fas fa-remove-format",
                    "hidden": true
                },
                "max_multiqc_email_size": {
                    "type": "string",
                    "description": "File size limit when attaching MultiQC reports to summary emails.",
                    "pattern": "^\\d+(\\.\\d+)?\\.?\\s*(K|M|G|T)?B$",
                    "default": "25.MB",
                    "fa_icon": "fas fa-file-upload",
                    "hidden": true
                },
                "monochrome_logs": {
                    "type": "boolean",
                    "description": "Do not use coloured log outputs.",
                    "fa_icon": "fas fa-palette",
                    "hidden": true
                },
                "multiqc_config": {
                    "type": "string",
                    "description": "Custom config file to supply to MultiQC.",
                    "fa_icon": "fas fa-cog",
                    "hidden": true
                },
                "tracedir": {
                    "type": "string",
                    "description": "Directory to keep pipeline Nextflow logs and reports.",
                    "default": "${params.outdir}/pipeline_info",
                    "fa_icon": "fas fa-cogs",
                    "hidden": true
                },
                "validate_params": {
                    "type": "boolean",
                    "description": "Boolean whether to validate parameters against the schema at runtime",
                    "default": true,
                    "fa_icon": "fas fa-check-square",
                    "hidden": true
                },
                "show_hidden_params": {
                    "type": "boolean",
                    "fa_icon": "far fa-eye-slash",
                    "description": "Show all params when using `--help`",
                    "hidden": true,
                    "help_text": "By default, parameters set as _hidden_ in the schema are not shown on the command line when a user runs with `--help`. Specifying this option will tell the pipeline to show all parameters."
                },
                "enable_conda": {
                    "type": "boolean",
                    "description": "Run this workflow with Conda. You can also use '-profile conda' instead of providing this parameter.",
                    "hidden": true,
                    "fa_icon": "fas fa-bacon"
                }
            }
        },
        "max_job_request_options": {
            "title": "Max job request options",
            "type": "object",
            "fa_icon": "fab fa-acquisitions-incorporated",
            "description": "Set the top limit for requested resources for any single job.",
            "help_text": "If you are running on a smaller system, a pipeline step requesting more resources than are available may cause the Nextflow to stop the run with an error. These options allow you to cap the maximum resources requested by any single job so that the pipeline will run on your system.\n\nNote that you can not _increase_ the resources requested by any job using these options. For that you will need your own configuration file. See [the nf-core website](https://nf-co.re/usage/configuration) for details.",
            "properties": {
                "max_cpus": {
                    "type": "integer",
                    "description": "Maximum number of CPUs that can be requested    for any single job.",
                    "default": 16,
                    "fa_icon": "fas fa-microchip",
                    "hidden": true,
                    "help_text": "Use to set an upper-limit for the CPU requirement for each process. Should be an integer e.g. `--max_cpus 1`"
                },
                "max_memory": {
                    "type": "string",
                    "description": "Maximum amount of memory that can be requested for any single job.",
                    "default": "128.GB",
                    "fa_icon": "fas fa-memory",
                    "pattern": "^\\d+(\\.\\d+)?\\.?\\s*(K|M|G|T)?B$",
                    "hidden": true,
                    "help_text": "Use to set an upper-limit for the memory requirement for each process. Should be a string in the format integer-unit e.g. `--max_memory '8.GB'`"
                },
                "max_time": {
                    "type": "string",
                    "description": "Maximum amount of time that can be requested for any single job.",
                    "default": "240.h",
                    "fa_icon": "far fa-clock",
                    "pattern": "^(\\d+\\.?\\s*(s|m|h|day)\\s*)+$",
                    "hidden": true,
                    "help_text": "Use to set an upper-limit for the time requirement for each process. Should be a string in the format integer-unit e.g. `--max_time '2.h'`"
                }
            }
        }
    },
    "allOf": [
        {
            "$ref": "#/definitions/main_arguments"
        },
        {
            "$ref": "#/definitions/other_input_output_options"
        },
        {
            "$ref": "#/definitions/cutoffs"
        },
        {
            "$ref": "#/definitions/taxonomic_database"
        },
        {
            "$ref": "#/definitions/filtering"
        },
        {
            "$ref": "#/definitions/skipping_specific_steps"
        },
        {
            "$ref": "#/definitions/generic_options"
        },
        {
            "$ref": "#/definitions/institutional_config_options"
        },
        {
            "$ref": "#/definitions/max_job_request_options"
        }
    ]
}
